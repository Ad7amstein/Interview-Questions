--- 
layout: post
title: What metrics can be used for evaluating classification models?
# subtitle: Key performance metrics to assess classification models effectively
gh-repo: Ad7amstein/Interview-Questions
gh-user: Ad7amstein
gh-badge: [star, fork, follow]
tags: [ml, supervised, classification, evaluation]
comments: true
mathjax: true
author: Adham Allam
profile-link: https://www.linkedin.com/in/adham-allam/
--- 

## Common Metrics  

When evaluating classification models, several key metrics are used depending on the task and class distribution:  

- **Accuracy**:  
  Fraction of correctly classified samples. Best when classes are balanced.  
  $$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$  

- **Precision**:  
  Out of all predicted positives, how many are actually positive.  
  $$Precision = \frac{TP}{TP + FP}$$  

- **Recall (Sensitivity or True Positive Rate)**:  
  Out of all actual positives, how many were correctly predicted.  
  $$Recall = \frac{TP}{TP + FN}$$  

- **F1-Score**:  
  Harmonic mean of Precision and Recall, useful for imbalanced datasets.  
  $$F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$  

- **Specificity (True Negative Rate)**:  
  Ability to identify negatives correctly.  
  $$Specificity = \frac{TN}{TN + FP}$$  

- **ROC-AUC (Receiver Operating Characteristic â€“ Area Under Curve)**:  
  Measures the ability of a model to distinguish between classes across thresholds.  

- **PR-AUC (Precision-Recall AUC)**:  
  More informative than ROC when dealing with imbalanced datasets.  

- **Log Loss (Cross-Entropy Loss)**:  
  Evaluates the uncertainty of probability predictions. Lower is better.  

## Example Table of Metrics  

| Metric       | Formula                              | When to Use |
|--------------|--------------------------------------|-------------|
| Accuracy     | $$\frac{TP + TN}{TP + TN + FP + FN}$$  | Balanced classes |
| Precision    | $$\frac{TP}{TP + FP}$$              | Minimize false positives |
| Recall       | $$\frac{TP}{TP + FN}$$              | Minimize false negatives |
| F1-Score     | $$2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$   | Imbalanced classes |
| ROC-AUC      | Area under ROC curve                | General binary classification |
| PR-AUC       | Area under Precision-Recall curve   | Highly imbalanced datasets |

### Notification  

{: .box-note}  
**Note:** The choice of metric depends heavily on the problem. For example, in medical diagnosis, *Recall* is often more important than Accuracy.  

### Warning  

{: .box-warning}  

**Warning:** Using Accuracy alone on imbalanced datasets can be misleading.
